import numpy as np
import pandas as pd
from dtreeviz.trees import dtreeviz
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier,
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, classification_report
from sklearn import svm
from sklearn.metrics import confusion_matrix
from sklearn.inspection import plot_partial_dependence, permutation_importance
import seaborn as sns
import matplotlib.pyplot as plt
from dtreeviz.trees import dtreeviz
from sklearn.svm import SVC
from sklearn import metrics     
from sklearn.neighbors import KNeighborsClassifier
%matplotlib inline

x_train_class, x_test_class, y_train_class, y_test_class = train_test_split(X, y_class, random_state=100, test_size=0.33)
#X contains explanatory variables and y_class is objective variable

param = {'n_estimators':[10, 100, 150, 300, ],
         'max_depth':[1, 2, 3, 4, 5],
         'max_features':[1, 2, 3, 4, 5],
         'min_samples_leaf':[1, 2, 3, 4, 5, 6, ],
         'min_samples_split':[2, 3, 4, 5]}

clf = GridSearchCV(RandomForestClassifier(),  
                   param, cv=3, iid=True)
clf.fit(x_train_class, y_train_class)
best_clf = clf.best_estimator_
print('Best_condition:\n', best_clf)
print('score_of_train_data:\n', best_clf.score(x_train_class, y_train_class))
print('score_of_test_data:\n', best_clf.score(x_test_class, y_test_class))

rfr = RandomForestClassifier(n_estimators = 150, max_depth=6, max_features=3, random_state=100)
rfr.fit(x_train_class, y_train_class)

print(rfr.score(x_train_class, y_train_class))

feature = rfr.feature_importances_
label = x_train_class.columns[0:]
indices = np.argsort(feature)[::1]

# plot
x = range(len(feature))
y = feature[indices]
y_label = label[indices]
plt.barh(x, y, align = 'center')
plt.yticks(x, y_label)
plt.xlabel("importance_num")
plt.ylabel("label")
plt.show()

predict_two_class = rfr.predict(x_test_class)

plt.figure(figsize=(7, 7))
cm_two_class = confusion_matrix(y_test_class, predict_two_class)
cm_two_class = np.flip(cm_two_class)
# cm = np.insert(cm, 6, 0, axis=0)
# zero= np.array([0, 0,0,0,0,0,0]).reshape(7,1)
# cm = np.append(cm, zero, axis=1)
cm_two_class = pd.DataFrame(data=cm_two_class, index=["≤ 15dB", "> 15dB"], 
                           columns=["≤ 15dB", "> 15dB"])
plot = sns.heatmap(cm_two_class, square=True, cbar=True, annot=True, cmap='Blues')
plt.title('Machine Learning Model',  fontsize=20)
plt.xlabel('Predicted AB gap', fontsize=14)
plt.ylabel('Postoperateve AB gap', fontsize=14)
plt.xticks(rotation=0)
plt.yticks(rotation=0)

x_2_train, x_2_test, y_2_train, y_2_test = train_test_split(X, y_rank, random_state=100, test_size=0.33)
#y_rank is objective variables which audio data was classified into seven categories

rfr_2 = RandomForestClassifier(n_estimators = 100, max_depth=6, max_features=4, random_state=100)
rfr_2.fit(x_2_train, y_2_train)
print(rfr_2.score(x_2_test, y_2_test))
print(rfr_2.score(x_2_train,y_2_train))
scores_rank_2 = cross_val_score(rfr_2, X, y_rank, cv=3)
pred_2 = rfr_2.predict(x_2_test)
print(scores_rank_2)

plt.figure(figsize=(7, 7))
cm = confusion_matrix(y_2_test, pred_2)
cm = pd.DataFrame(data=cm, index=["≤ 0dB", "≤ 10dB", "≤ 20dB", "≤ 30dB", "≤ 40dB", "≤ 50dB", "≤ 60dB"], 
                           columns=["≤ 0dB", "≤ 10dB", "≤ 20dB", "≤ 30dB", "≤ 40dB", "≤ 50dB", "≤ 60dB"])
plot = sns.heatmap(cm, square=True, cbar=True, annot=True, cmap='Blues')
plt.title('Machine Learning Model',  fontsize=20)
plt.xlabel('Predicted AB gap', fontsize=14)
plt.ylabel('Postoperateve AB gap', fontsize=14)
plt.xticks(rotation=0)
plt.yticks(rotation=0)

feature = rfr_2.feature_importances_
label = x_2_train.columns[0:]
indices = np.argsort(feature)[::1]

x = range(len(feature))
y = feature[indices]
y_label = label[indices]
plt.barh(x, y, align = 'center')
plt.yticks(x, y_label)
plt.xlabel("importance_num")
plt.ylabel("label")

plt.show()
